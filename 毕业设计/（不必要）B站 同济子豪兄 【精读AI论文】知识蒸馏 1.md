（2025年03月21日）
distil即把大模型teacher的知识提取到student模型中。
	数学上：把大的、复杂的网格转换为小、轻量的网格。（想法：可能有没有学到的
	目的：算力有限。（如果仅仅如此，对我无用呢
预训练大模型（大算力训练，多参数），经过蒸馏后部署到设备::算力有限。
**轻量化知识网络：** 
- （无关）压缩已有模型：知识蒸馏、权值量化（16f 8f）、%%剪枝、注意力迁移%%
- （或许）直接训练轻量化：squeezeNet等
- （无关）加速卷积运算：im2col+GEMM, Winograd, 转换到硬件优化的计算方法
- （无关）硬件部署：TensorRT, Jetson开发板（800元）：硬件和软件交互
- 等等许多方法
#### 知识的表达与迁移
MNIST只告诉了某个图片‘是某个数字’而没有告知‘不是其他数字’，因此‘不是其他数字’的程序‘被认为是相同的’。由此，预测概率必须从hard targets转换为soft targets以软化我们的判断。soft targets不能准确判断‘对人类显然的’，却能判断‘对人类不显然的’。
使用soft targets作为标签训练student模型。使用softmax在特定蒸馏温度下，使student和teacher接近；使用softmax在T=1，使student和hard targets接近。
	接近的反向，就是损失函数。
	student似乎做了：1.从教师学习‘大概是’的分类 2.做题，只有正误
损失函数最小化，就是目的。
**过拟合：** 小训练集，训练集准确度高，测试集准确度低。发展是先增后降的。
**零样本：** ‘3’的知识迁移到没见过‘3’的student
**为什么？** （或许是：teacher网络收敛位置和student，由于空间不同大小，所以收敛位置不同；如果有teacher-guided就能收敛到接近位置）
#### pytorch实现